# Self-Refining LLM Chat System

A prototype chat system that combines **LM Studio**, **SearXNG** to create a self-refining retrieval-augmented generation (RAG) loop.  
The system uses an LLM both as a **worker** (search + summarize) and as a **supervisor** (review + refine), iterating until the response meets quality standards.

---

## âœ¨ Features
- **Intent Analysis**: LLM interprets user requests and generates search keywords.
- **Search Integration**: Queries SearXNG (`http://localhost:8888`) for fresh results.
- **Summarization**: LLM condenses retrieved information into clear answers.
- **Self-Review Loop**: LLM critiques its own output and refines queries until results are sufficient.
- **Tool Integration**: Docker MCP provides external capabilities (scraping, file access, APIs).

---

## ğŸ—ï¸ Architecture
1. **LM Studio** â€“ Local LLM server (`http://localhost:1234`) running `openai/gpt-oss-20b`.
2. **SearXNG** â€“ Local meta-search engine (`http://localhost:8888`) with JSON API.
3. **Controller Script** â€“ Python orchestrator that manages the loop.

---

# Self-Refining LLM Chat Orchestration

A modular, multiâ€‘role orchestration framework for large language models (LLMs).  
This project demonstrates how to coordinate specialized roles (Planner, Decomposer, Collector, Editor, Auditor, Specialist, Supervisor, Fulfillment Checker, Critical Thinker, Integrator) into a reproducible pipeline that produces professional, evidenceâ€‘driven reports.

---

## âœ¨ Features

- **Roleâ€‘based modular design**  
  Each role lives in its own Python module (`roles/`), making it easy to extend or swap logic independently.

- **Agentic RAG (Retrievalâ€‘Augmented Generation)**  
  Evidence collection from web search (via SearXNG), deep fetch, and local file ingestion.  
  Adaptive semantic compression ensures factual detail is preserved while reducing token usage.

- **Iterative refinement loop**  
  Multiâ€‘round drafting, auditing, enrichment, scoring, and fulfillment checks until quality thresholds are met.

- **Executive summary & integration**  
  Automatic generation of boardâ€‘ready summaries and integrated Markdown reports, preserving diagrams (Mermaid syntax).

- **Transparency & auditability**  
  - Iteration history saved (`history.json`)  
  - Evidence pool persisted (`evidence_pool.json`)  
  - Token usage tracked per role  
  - Logs written to both file (`sketch_v4.log`) and console

- **Monitoring & feedback**  
  - Nested progress bars (`tqdm`) for rounds and sections  
  - Colorâ€‘coded console summaries (green/yellow/red) for quality  
  - Final outcome banner + token usage summary

---

## ğŸ“‚ Project Structure

```
sketch_v4/
â”‚
â”œâ”€â”€ main.py               # CLI harness
â”œâ”€â”€ orchestrator.py       # Orchestrator class
â”‚
â”œâ”€â”€ roles/                # Role modules
â”‚   â”œâ”€â”€ planner.py
â”‚   â”œâ”€â”€ decomposer.py
â”‚   â”œâ”€â”€ collector.py
â”‚   â”œâ”€â”€ editor.py
â”‚   â”œâ”€â”€ auditor.py
â”‚   â”œâ”€â”€ specialist.py
â”‚   â”œâ”€â”€ supervisor.py
â”‚   â”œâ”€â”€ fulfillment.py
â”‚   â”œâ”€â”€ critical.py
â”‚   â””â”€â”€ integrator.py
â”‚
â””â”€â”€ utils/                # Utilities
    â”œâ”€â”€ config.py
    â”œâ”€â”€ helpers.py
    â”œâ”€â”€ token_counter.py
    â”œâ”€â”€ persistence.py
    â””â”€â”€ logging_utils.py
```

---

## ğŸš€ Quick Start

1. **Clone the repo**
   ```bash
   git clone https://github.com/yourusername/self-refining-llm-chat.git
   cd self-refining-llm-chat
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   Required packages: `requests`, `tqdm`, `colorama`, `dataclasses` (Python 3.7+ includes it).

3. **Run the CLI harness**
   ```bash
   python main.py
   ```

4. **Monitor progress**
   - Console: progress bars + colorâ€‘coded summaries  
   - Log file: `sketch_v4.log`  

5. **Check outputs**
   - `final_report.md` â†’ integrated professional report  
   - `executive_summary.md` â†’ boardâ€‘ready summary  
   - `history.json` â†’ iteration history  
   - `evidence_pool.json` â†’ collected evidence  
   - `manifest.json` â†’ artifact manifest  

---

## ğŸ›  Configuration

Edit `utils/config.py` to adjust:
- `LM_STUDIO_URL` â†’ local LM Studio endpoint  
- `SEARX_URL` â†’ SearXNG search endpoint  
- `LLM_CFG` â†’ max tokens, timeout  
- `ROLE_TEMPS` â†’ perâ€‘role temperature settings  

---

## ğŸ“Š Example Output

Console:

```
Orchestration rounds:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 1/3 [00:15<00:30, 15.0s/round]
Round 1 sections:     100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.00s/section]
Round 1 summary: avg_overall=7.25, improvements=2, tokens_used=450   â† (yellow text)

============================================================
âš ï¸ REVIEW: Report acceptable but improvements suggested
============================================================

============================================================
ğŸ“Š TOKEN USAGE SUMMARY
============================================================
Total tokens used: 1450
Per-role breakdown:
  planner      prompt=120 completion=300 total=420
  editor       prompt=200 completion=400 total=600
  supervisor   prompt=80  completion=10  total=90
```

---

## ğŸ¤ Contributing

This project is open to requests and contributions.  
Ideas welcome for:
- New specialized roles (e.g. Scenario Mapper, Diagram Generator)  
- Improved scoring rubrics  
- Alternative evidence sources  
- Visualization integration  

Fork, open issues, or submit PRs â€” letâ€™s refine this orchestration together.
